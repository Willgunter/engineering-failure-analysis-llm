{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXc6LxklTJ8q"
   },
   "source": [
    "correct order:\n",
    "1) switch to gpu\n",
    "2) mount colab runtime\n",
    "4) pip install commands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 6801,
     "status": "ok",
     "timestamp": 1769181541881,
     "user": {
      "displayName": "Will Gunter",
      "userId": "04793422759907679153"
     },
     "user_tz": 300
    },
    "id": "VWnbR4RrQ_Lp",
    "outputId": "584945ac-da92-48df-a4d4-7e754017e485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 153028,
     "status": "ok",
     "timestamp": 1769181744188,
     "user": {
      "displayName": "Will Gunter",
      "userId": "04793422759907679153"
     },
     "user_tz": 300
    },
    "id": "SGR8FYjyTC8w",
    "outputId": "06ec82df-f56d-4100-fbe5-53eb138be7e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m405.7/405.7 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m310.8/310.8 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.9/122.9 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m135.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install -U transformers datasets accelerate peft bitsandbytes unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1769181776207,
     "user": {
      "displayName": "Will Gunter",
      "userId": "04793422759907679153"
     },
     "user_tz": 300
    },
    "id": "Wts97kPzUtM1"
   },
   "outputs": [],
   "source": [
    "model_to_use = \"Qwen/Qwen3-0.6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 979504,
     "status": "ok",
     "timestamp": 1769183778158,
     "user": {
      "displayName": "Will Gunter",
      "userId": "04793422759907679153"
     },
     "user_tz": 300
    },
    "id": "MQBZ8UVsVOzY",
    "outputId": "6b3a0889-bd8e-4ae2-e6c3-106a424a92fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "2026-01-23 15:40:03.362652: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-23 15:40:03.381559: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769182803.403831    9619 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769182803.411160    9619 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1769182803.430409    9619 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769182803.430437    9619 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769182803.430440    9619 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769182803.430443    9619 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-23 15:40:03.435325: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Raw text length (chars): 591990\n",
      "Split token: '---WILLGPTSTART---'\n",
      "Split token occurrences: 847\n",
      "Loaded 847 examples\n",
      "Sample parsed entries (up to 2):\n",
      "instruction preview: Answer the engineering question in 3â€“8 short lines. Rules:- Do NOT ask follow-up questions.- Do NOT generate multiple-choice options (no A), B), a), b), etc).- Do NOT include 'Answer:' or 'Q:' or 'A:'\n",
      "response preview: Q: Which underlying assumptions most weaken the claim that motor heaters won't nuisance-trip during low bus voltage?\n",
      "\n",
      "A:\n",
      "\n",
      "assumes low-voltage episodes won't persist longer than the heaterâ€™s thermal in\n",
      "---\n",
      "instruction preview: Answer the engineering question in 3â€“8 short lines. Rules:- Do NOT ask follow-up questions.- Do NOT generate multiple-choice options (no A), B), a), b), etc).- Do NOT include 'Answer:' or 'Q:' or 'A:'\n",
      "response preview: Q: What setpoint or coordination choices create single-point failures between avoiding nuisance trips and maintaining fault protection?\n",
      "\n",
      "A:\n",
      "\n",
      "raising instantaneous breaker thresholds to tolerate higher\n",
      "---\n",
      "==((====))==  Unsloth 2026.1.4: Fast Qwen3 patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2026.1.4 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n",
      "Map: 100% 847/847 [00:01<00:00, 744.77 examples/s]\n",
      "Tokenized lengths: min=202, median=300, max=429\n",
      "First 10 lengths: [202, 228, 229, 231, 234, 234, 235, 236, 237, 238]\n",
      "Last 10 lengths: [403, 403, 405, 408, 408, 409, 410, 414, 415, 429]\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 847 | Num Epochs = 4 | Total steps = 1,696\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 1 x 1) = 2\n",
      " \"-____-\"     Trainable parameters = 80,740,352 of 676,790,272 (11.93% trained)\n",
      "{'loss': 4.1276, 'grad_norm': 4.061174392700195, 'learning_rate': 0.0002913325471698113, 'epoch': 0.12}\n",
      "{'loss': 3.9207, 'grad_norm': 4.32944393157959, 'learning_rate': 0.0002824882075471698, 'epoch': 0.24}\n",
      "{'loss': 3.7656, 'grad_norm': 4.039172172546387, 'learning_rate': 0.00027364386792452826, 'epoch': 0.35}\n",
      "{'loss': 3.712, 'grad_norm': 4.786346435546875, 'learning_rate': 0.00026479952830188674, 'epoch': 0.47}\n",
      "{'loss': 3.7108, 'grad_norm': 6.324872970581055, 'learning_rate': 0.0002559551886792453, 'epoch': 0.59}\n",
      "{'loss': 3.7117, 'grad_norm': 3.9513304233551025, 'learning_rate': 0.00024711084905660376, 'epoch': 0.71}\n",
      "{'loss': 3.6542, 'grad_norm': 3.9275808334350586, 'learning_rate': 0.00023826650943396224, 'epoch': 0.83}\n",
      "{'loss': 3.6068, 'grad_norm': 5.501747131347656, 'learning_rate': 0.00022942216981132075, 'epoch': 0.94}\n",
      "{'loss': 3.0796, 'grad_norm': 3.9192678928375244, 'learning_rate': 0.0002205778301886792, 'epoch': 1.06}\n",
      "{'loss': 2.6182, 'grad_norm': 4.662356853485107, 'learning_rate': 0.00021173349056603772, 'epoch': 1.18}\n",
      "{'loss': 2.6131, 'grad_norm': 3.899489641189575, 'learning_rate': 0.0002028891509433962, 'epoch': 1.3}\n",
      "{'loss': 2.6009, 'grad_norm': 4.639073371887207, 'learning_rate': 0.0001940448113207547, 'epoch': 1.42}\n",
      "{'loss': 2.624, 'grad_norm': 4.710995197296143, 'learning_rate': 0.00018520047169811322, 'epoch': 1.53}\n",
      "{'loss': 2.6026, 'grad_norm': 4.592080593109131, 'learning_rate': 0.00017635613207547167, 'epoch': 1.65}\n",
      "{'loss': 2.6267, 'grad_norm': 4.5575666427612305, 'learning_rate': 0.00016751179245283018, 'epoch': 1.77}\n",
      "{'loss': 2.5437, 'grad_norm': 3.9150772094726562, 'learning_rate': 0.00015866745283018866, 'epoch': 1.89}\n",
      "{'loss': 2.5225, 'grad_norm': 3.8697454929351807, 'learning_rate': 0.00014982311320754717, 'epoch': 2.0}\n",
      "{'loss': 1.4048, 'grad_norm': 6.51888370513916, 'learning_rate': 0.00014097877358490565, 'epoch': 2.12}\n",
      "{'loss': 1.2804, 'grad_norm': 4.971953392028809, 'learning_rate': 0.00013213443396226413, 'epoch': 2.24}\n",
      "{'loss': 1.2801, 'grad_norm': 5.283240795135498, 'learning_rate': 0.00012329009433962264, 'epoch': 2.36}\n",
      "{'loss': 1.3376, 'grad_norm': 5.589030742645264, 'learning_rate': 0.00011444575471698112, 'epoch': 2.48}\n",
      "{'loss': 1.3589, 'grad_norm': 5.295757293701172, 'learning_rate': 0.0001056014150943396, 'epoch': 2.59}\n",
      "{'loss': 1.3047, 'grad_norm': 5.202409267425537, 'learning_rate': 9.67570754716981e-05, 'epoch': 2.71}\n",
      "{'loss': 1.3279, 'grad_norm': 4.494056224822998, 'learning_rate': 8.791273584905661e-05, 'epoch': 2.83}\n",
      "{'loss': 1.244, 'grad_norm': 5.81198787689209, 'learning_rate': 7.906839622641509e-05, 'epoch': 2.95}\n",
      "{'loss': 0.8065, 'grad_norm': 2.5695416927337646, 'learning_rate': 7.022405660377359e-05, 'epoch': 3.07}\n",
      "{'loss': 0.4291, 'grad_norm': 4.449112892150879, 'learning_rate': 6.137971698113207e-05, 'epoch': 3.18}\n",
      "{'loss': 0.4237, 'grad_norm': 4.090498924255371, 'learning_rate': 5.253537735849056e-05, 'epoch': 3.3}\n",
      "{'loss': 0.421, 'grad_norm': 4.355676651000977, 'learning_rate': 4.369103773584905e-05, 'epoch': 3.42}\n",
      "{'loss': 0.4124, 'grad_norm': 3.6950855255126953, 'learning_rate': 3.484669811320754e-05, 'epoch': 3.54}\n",
      "{'loss': 0.4058, 'grad_norm': 4.6468281745910645, 'learning_rate': 2.6002358490566035e-05, 'epoch': 3.66}\n",
      "{'loss': 0.3878, 'grad_norm': 3.4328038692474365, 'learning_rate': 1.7158018867924527e-05, 'epoch': 3.77}\n",
      "{'loss': 0.3998, 'grad_norm': 2.9352362155914307, 'learning_rate': 8.313679245283019e-06, 'epoch': 3.89}\n",
      "{'train_runtime': 924.1188, 'train_samples_per_second': 3.666, 'train_steps_per_second': 1.835, 'train_loss': 2.0234971698725, 'epoch': 4.0}\n",
      "100% 1696/1696 [15:24<00:00,  1.84it/s]\n"
     ]
    }
   ],
   "source": [
    "!python drive/MyDrive/AI/willgpt/finetune1/attempt2/finetune1_script_attempt2.py --model=\"{model_to_use}\" \\\n",
    "  --data-path /content/drive/MyDrive/AI/willgpt/finetune1/attempt2/finetune1_formatted_data_attempt2.txt \\\n",
    "  --debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 30456,
     "status": "ok",
     "timestamp": 1769184018932,
     "user": {
      "displayName": "Will Gunter",
      "userId": "04793422759907679153"
     },
     "user_tz": 300
    },
    "id": "Tv8F-c6vAqrM",
    "outputId": "83e3d614-f104-40c6-985e-f10acb829142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-23 15:59:54.058996: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-23 15:59:54.078191: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769183994.100673   14846 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769183994.108006   14846 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1769183994.127095   14846 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769183994.127122   14846 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769183994.127125   14846 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769183994.127128   14846 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-01-23 15:59:54.132021: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "tokenizer_config.json: 9.73kB [00:00, 38.6MB/s]\n",
      "vocab.json: 2.78MB [00:00, 4.95MB/s]\n",
      "merges.txt: 1.67MB [00:00, 137MB/s]\n",
      "tokenizer.json: 100% 11.4M/11.4M [00:01<00:00, 9.44MB/s]\n",
      "config.json: 100% 726/726 [00:00<00:00, 7.34MB/s]\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "model.safetensors: 100% 1.50G/1.50G [00:06<00:00, 223MB/s]\n",
      "generation_config.json: 100% 239/239 [00:00<00:00, 2.51MB/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/peft/config.py\", line 317, in _get_peft_type\n",
      "    config_file = hf_hub_download(\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n",
      "    validate_repo_id(arg_value)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 160, in validate_repo_id\n",
      "    raise HFValidationError(\n",
      "huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars, '-', '_' or '.'. The name cannot start or end with '-' or '.' and the maximum length is 96: './qwen3-0.6B-extreme-reasoning-lora'.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/drive/MyDrive/AI/willgpt/evals/custom-evals/run_custom_eval.py\", line 273, in <module>\n",
      "    main()\n",
      "  File \"/content/drive/MyDrive/AI/willgpt/evals/custom-evals/run_custom_eval.py\", line 179, in main\n",
      "    peft_model = PeftModel.from_pretrained(base_model, args.adapter_path)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\", line 459, in from_pretrained\n",
      "    config = PEFT_TYPE_TO_CONFIG_MAPPING[PeftConfig._get_peft_type(model_id, **hf_kwargs)].from_pretrained(\n",
      "                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/peft/config.py\", line 323, in _get_peft_type\n",
      "    raise ValueError(f\"Can't find '{CONFIG_NAME}' at '{model_id}'\")\n",
      "ValueError: Can't find 'adapter_config.json' at './qwen3-0.6B-extreme-reasoning-lora'\n"
     ]
    }
   ],
   "source": [
    "!python /content/drive/MyDrive/AI/willgpt/evals/custom-evals/run_custom_eval.py \\\n",
    "  --prompts /content/drive/MyDrive/AI/willgpt/evals/custom-evals/prompts.jsonl \\\n",
    "  --adapter-path ./qwen3-0.6B-extreme-reasoning-lora \\\n",
    "  --model=\"{model_to_use}\" \\\n",
    "  --max-new-tokens 128 \\\n",
    "  --batch-size 16 \\\n",
    "  --trust-remote-code\n",
    "  #  note to self: make sure you test base model w/ new params as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1325,
     "status": "ok",
     "timestamp": 1769184023185,
     "user": {
      "displayName": "Will Gunter",
      "userId": "04793422759907679153"
     },
     "user_tz": 300
    },
    "id": "n_ESRqjPWOPx"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from huggingface_hub import HfApi\n",
    "login(token=\"INSERT_TOKEN_HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2559,
     "status": "ok",
     "timestamp": 1769184027111,
     "user": {
      "displayName": "Will Gunter",
      "userId": "04793422759907679153"
     },
     "user_tz": 300
    },
    "id": "llcKUeEyXlf1",
    "outputId": "63b1db6e-4d0e-4117-d017-ec3f3ae80443"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe --type argument is deprecated and will be removed in a future version. Use --repo-type instead.\u001b[0m\n",
      "\u001b[33mâš ï¸  Warning: 'huggingface-cli repo' is deprecated. Use 'hf repo' instead.\u001b[0m\n",
      "Successfully created \u001b[1mWillgunter/qwen3-0.6b-extreme-conditions-lora\u001b[0m on the Hub.\n",
      "Your repo is now available at \u001b[1mhttps://huggingface.co/Willgunter/qwen3-0.6b-extreme-conditions-lora\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli repo create qwen3-0.6b-extreme-conditions-lora --type model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21208,
     "status": "ok",
     "timestamp": 1769184161297,
     "user": {
      "displayName": "Will Gunter",
      "userId": "04793422759907679153"
     },
     "user_tz": 300
    },
    "id": "eJrtqf7rYl4k",
    "outputId": "a9b1ddb2-7fe3-4a6f-f356-3dbcdf5f999d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mâš ï¸  Warning: 'huggingface-cli upload' is deprecated. Use 'hf upload' instead.\u001b[0m\n",
      "Start hashing 10 files.\n",
      "Finished hashing 10 files.\n",
      "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n",
      "New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:   0% 46.7k/323M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :   0% 46.7k/334M [00:02<4:00:04, 23.2kB/s, 25.9kB/s  ]\n",
      "\n",
      "  ...adapter_model.safetensors:   0% 46.7k/323M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:   0% 46.7k/323M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:   0% 46.7k/323M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:   0% 46.7k/323M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...-qwen-lora/tokenizer.json: 100% 11.4M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:   0% 604k/323M [00:00<09:38, 557kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :   4% 12.0M/334M [00:03<01:04, 4.99MB/s, 4.30MB/s  ]  \n",
      "New Data Upload               :   0% 557k/134M [00:03<12:01, 185kB/s,  199kB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:   0% 604k/323M [00:01<11:34, 464kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...-qwen-lora/tokenizer.json: 100% 11.4M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:   0% 1.16M/323M [00:01<06:44, 795kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :   4% 12.6M/334M [00:03<01:14, 4.30MB/s, 3.93MB/s  ]\n",
      "New Data Upload               :   1% 1.11M/134M [00:03<05:52, 377kB/s,  348kB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:   0% 1.16M/323M [00:01<07:42, 696kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...-qwen-lora/tokenizer.json: 100% 11.4M/11.4M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:   1% 2.27M/323M [00:01<04:19, 1.24MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :   4% 13.7M/334M [00:03<01:20, 3.97MB/s, 3.80MB/s  ]\n",
      "New Data Upload               :   2% 2.23M/134M [00:03<02:39, 824kB/s,  619kB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:   1% 3.94M/323M [00:01<02:43, 1.95MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :   5% 15.4M/334M [00:04<01:09, 4.56MB/s, 4.04MB/s  ]\n",
      "New Data Upload               :   3% 3.90M/134M [00:04<01:16, 1.71MB/s, 1.03MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:   2% 6.73M/323M [00:02<01:44, 3.04MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :   5% 18.2M/334M [00:04<00:52, 6.06MB/s, 4.54MB/s  ]\n",
      "New Data Upload               :   3% 6.68M/201M [00:04<00:56, 3.46MB/s, 1.67MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:   3% 9.51M/323M [00:02<01:19, 3.94MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :   6% 20.9M/334M [00:04<00:41, 7.51MB/s, 4.98MB/s  ]\n",
      "New Data Upload               :   5% 9.46M/201M [00:04<00:36, 5.23MB/s, 2.25MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:   3% 10.1M/323M [00:02<01:21, 3.85MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :   6% 21.5M/334M [00:04<00:47, 6.52MB/s, 4.88MB/s  ]\n",
      "New Data Upload               :   4% 10.0M/268M [00:04<00:54, 4.75MB/s, 2.28MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:   5% 15.1M/323M [00:02<00:57, 5.37MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :   8% 26.5M/334M [00:04<00:28, 10.8MB/s, 5.76MB/s  ]\n",
      "New Data Upload               :   6% 15.0M/268M [00:04<00:27, 9.18MB/s, 3.27MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:   6% 20.1M/323M [00:02<00:45, 6.68MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :   9% 31.5M/334M [00:05<00:21, 14.3MB/s, 6.56MB/s  ]\n",
      "New Data Upload               :   7% 20.0M/268M [00:05<00:19, 12.9MB/s, 4.17MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:   8% 25.7M/323M [00:03<00:37, 8.00MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  11% 37.1M/334M [00:05<00:16, 17.9MB/s, 7.41MB/s  ]\n",
      "New Data Upload               :  10% 25.6M/268M [00:05<00:14, 16.7MB/s, 5.12MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  11% 34.6M/323M [00:03<00:28, 10.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  14% 46.0M/334M [00:05<00:11, 25.1MB/s, 8.84MB/s  ]\n",
      "New Data Upload               :  13% 34.5M/268M [00:05<00:09, 24.1MB/s, 6.64MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  13% 41.2M/323M [00:03<00:24, 11.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  16% 52.7M/334M [00:05<00:10, 27.4MB/s, 9.75MB/s  ]\n",
      "New Data Upload               :  15% 41.2M/268M [00:05<00:08, 26.7MB/s, 7.63MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  15% 47.4M/323M [00:03<00:22, 12.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  18% 58.8M/334M [00:05<00:09, 28.3MB/s, 10.5MB/s  ]\n",
      "New Data Upload               :  15% 47.3M/323M [00:05<00:09, 27.8MB/s, 8.45MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  17% 54.6M/323M [00:03<00:19, 13.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  20% 66.0M/334M [00:06<00:08, 30.6MB/s, 11.4MB/s  ]\n",
      "New Data Upload               :  17% 54.5M/323M [00:06<00:08, 30.2MB/s, 9.40MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  19% 62.9M/323M [00:04<00:17, 15.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  22% 74.4M/334M [00:06<00:07, 33.9MB/s, 12.4MB/s  ]\n",
      "New Data Upload               :  19% 62.9M/323M [00:06<00:07, 33.5MB/s, 10.5MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  23% 74.6M/323M [00:04<00:14, 17.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  26% 86.0M/334M [00:06<00:06, 41.1MB/s, 13.9MB/s  ]\n",
      "New Data Upload               :  23% 74.6M/323M [00:06<00:06, 40.8MB/s, 12.0MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  27% 86.3M/323M [00:04<00:12, 18.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  29% 97.7M/334M [00:06<00:05, 46.2MB/s, 15.3MB/s  ]\n",
      "New Data Upload               :  27% 86.3M/323M [00:06<00:05, 46.0MB/s, 13.5MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  30% 96.3M/323M [00:04<00:11, 20.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  32% 108M/334M [00:06<00:04, 47.4MB/s, 16.3MB/s  ] \n",
      "New Data Upload               :  30% 96.3M/323M [00:06<00:04, 47.3MB/s, 14.6MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  33% 108M/323M [00:04<00:09, 21.6MB/s] \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  36% 119M/334M [00:07<00:04, 50.7MB/s, 17.6MB/s  ]\n",
      "New Data Upload               :  33% 108M/323M [00:07<00:04, 50.6MB/s, 15.9MB/s  ] \u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  36% 116M/323M [00:05<00:09, 22.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  38% 128M/334M [00:07<00:04, 48.0MB/s, 18.3MB/s  ]\n",
      "New Data Upload               :  36% 116M/323M [00:07<00:04, 47.9MB/s, 16.6MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  40% 129M/323M [00:05<00:08, 23.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  42% 140M/334M [00:07<00:03, 52.0MB/s, 19.4MB/s  ]\n",
      "New Data Upload               :  40% 129M/323M [00:07<00:03, 51.9MB/s, 17.9MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  44% 142M/323M [00:05<00:07, 25.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  46% 153M/334M [00:07<00:03, 56.4MB/s, 20.7MB/s  ]\n",
      "New Data Upload               :  44% 142M/323M [00:07<00:03, 56.4MB/s, 19.2MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  48% 156M/323M [00:05<00:06, 27.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  50% 168M/334M [00:07<00:02, 61.1MB/s, 22.1MB/s  ]\n",
      "New Data Upload               :  48% 156M/323M [00:07<00:02, 61.1MB/s, 20.6MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  53% 171M/323M [00:05<00:05, 28.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  55% 183M/334M [00:08<00:02, 65.3MB/s, 23.4MB/s  ]\n",
      "New Data Upload               :  53% 171M/323M [00:08<00:02, 65.3MB/s, 22.0MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  58% 188M/323M [00:06<00:04, 30.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  60% 199M/334M [00:08<00:01, 70.0MB/s, 24.9MB/s  ]\n",
      "New Data Upload               :  58% 188M/323M [00:08<00:01, 69.9MB/s, 23.4MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  63% 203M/323M [00:06<00:03, 31.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  64% 214M/334M [00:08<00:01, 71.6MB/s, 26.1MB/s  ]\n",
      "New Data Upload               :  63% 203M/323M [00:08<00:01, 71.6MB/s, 24.7MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  66% 214M/323M [00:06<00:03, 32.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  68% 226M/334M [00:08<00:01, 67.6MB/s, 26.9MB/s  ]\n",
      "New Data Upload               :  66% 214M/323M [00:08<00:01, 67.6MB/s, 25.5MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  70% 228M/323M [00:06<00:02, 33.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  71% 239M/334M [00:08<00:01, 67.3MB/s, 27.8MB/s  ]\n",
      "New Data Upload               :  70% 228M/323M [00:08<00:01, 67.3MB/s, 26.5MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  74% 238M/323M [00:06<00:02, 34.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  75% 250M/334M [00:09<00:01, 63.0MB/s, 28.4MB/s  ]\n",
      "New Data Upload               :  74% 238M/323M [00:09<00:01, 63.0MB/s, 27.1MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  78% 250M/323M [00:07<00:02, 34.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  78% 262M/334M [00:09<00:01, 62.5MB/s, 29.1MB/s  ]\n",
      "New Data Upload               :  78% 250M/323M [00:09<00:01, 62.5MB/s, 27.8MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  80% 258M/323M [00:07<00:01, 34.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  81% 270M/334M [00:09<00:01, 55.4MB/s, 29.3MB/s  ]\n",
      "New Data Upload               :  80% 258M/323M [00:09<00:01, 55.4MB/s, 28.1MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  83% 267M/323M [00:07<00:01, 35.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  83% 278M/334M [00:09<00:01, 51.4MB/s, 29.6MB/s  ]\n",
      "New Data Upload               :  83% 267M/323M [00:09<00:01, 51.4MB/s, 28.4MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  85% 276M/323M [00:07<00:01, 35.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  86% 288M/334M [00:09<00:00, 50.1MB/s, 29.9MB/s  ]\n",
      "New Data Upload               :  85% 276M/323M [00:09<00:00, 50.1MB/s, 28.8MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  89% 286M/323M [00:07<00:01, 35.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  89% 298M/334M [00:10<00:00, 50.1MB/s, 30.4MB/s  ]\n",
      "New Data Upload               :  89% 286M/323M [00:10<00:00, 50.1MB/s, 29.2MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  91% 293M/323M [00:08<00:00, 35.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  91% 305M/334M [00:10<00:00, 45.6MB/s, 30.5MB/s  ]\n",
      "New Data Upload               :  91% 293M/323M [00:10<00:00, 45.6MB/s, 29.3MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  94% 303M/323M [00:08<00:00, 36.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  94% 315M/334M [00:10<00:00, 47.2MB/s, 30.9MB/s  ]\n",
      "New Data Upload               :  94% 303M/323M [00:10<00:00, 47.2MB/s, 29.7MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  95% 308M/323M [00:08<00:00, 35.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  95% 319M/334M [00:10<00:00, 39.7MB/s, 31.3MB/s  ]\n",
      "New Data Upload               :  95% 308M/323M [00:10<00:00, 39.7MB/s, 30.2MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  97% 314M/323M [00:08<00:00, 35.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  97% 326M/334M [00:10<00:00, 37.8MB/s, 31.9MB/s  ]\n",
      "New Data Upload               :  97% 314M/323M [00:10<00:00, 37.8MB/s, 30.8MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors:  99% 318M/323M [00:08<00:00, 35.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      :  99% 330M/334M [00:11<00:00, 32.3MB/s, 32.3MB/s  ]\n",
      "New Data Upload               :  99% 318M/323M [00:11<00:00, 32.3MB/s, 31.2MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors: 100% 322M/323M [00:09<00:00, 35.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      : 100% 334M/334M [00:11<00:00, 28.5MB/s, 32.7MB/s  ]\n",
      "New Data Upload               : 100% 322M/323M [00:11<00:00, 28.5MB/s, 31.6MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors: 100% 322M/323M [00:09<00:00, 34.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...-qwen-lora/tokenizer.json: 100% 11.4M/11.4M [00:08<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors: 100% 322M/323M [00:09<00:00, 33.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...-qwen-lora/tokenizer.json: 100% 11.4M/11.4M [00:08<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors: 100% 322M/323M [00:09<00:00, 32.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...-qwen-lora/tokenizer.json: 100% 11.4M/11.4M [00:08<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors: 100% 323M/323M [00:09<00:00, 32.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      : 100% 334M/334M [00:12<00:00, 10.7MB/s, 32.7MB/s  ]\n",
      "New Data Upload               : 100% 323M/323M [00:12<00:00, 10.7MB/s, 31.6MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors: 100% 323M/323M [00:10<00:00, 31.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (1 / 2)      : 100% 334M/334M [00:12<00:00, 8.96MB/s, 32.8MB/s  ]\n",
      "New Data Upload               : 100% 323M/323M [00:12<00:00, 8.96MB/s, 31.6MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors: 100% 323M/323M [00:10<00:00, 31.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...-qwen-lora/tokenizer.json: 100% 11.4M/11.4M [00:09<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors: 100% 323M/323M [00:10<00:00, 30.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...-qwen-lora/tokenizer.json: 100% 11.4M/11.4M [00:09<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors: 100% 323M/323M [00:10<00:00, 29.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (2 / 2)      : 100% 334M/334M [00:12<00:00, 5.18MB/s, 32.8MB/s  ]\n",
      "New Data Upload               : 100% 323M/323M [00:12<00:00, 5.18MB/s, 31.7MB/s  ]\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors: 100% 323M/323M [00:10<00:00, 29.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...-qwen-lora/tokenizer.json: 100% 11.4M/11.4M [00:10<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors: 100% 323M/323M [00:11<00:00, 28.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...-qwen-lora/tokenizer.json: 100% 11.4M/11.4M [00:10<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors: 100% 323M/323M [00:11<00:00, 28.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  ...-qwen-lora/tokenizer.json: 100% 11.4M/11.4M [00:10<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  ...adapter_model.safetensors: 100% 323M/323M [00:11<00:00, 28.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing Files (2 / 2)      : 100% 334M/334M [00:13<00:00, 24.9MB/s, 32.2MB/s  ]\n",
      "New Data Upload               : 100% 323M/323M [00:13<00:00, 24.1MB/s, 32.2MB/s  ]\n",
      "  ...adapter_model.safetensors: 100% 323M/323M [00:11<00:00, 28.3MB/s]\n",
      "  ...-qwen-lora/tokenizer.json: 100% 11.4M/11.4M [00:10<?, ?B/s]\n",
      "https://huggingface.co/Willgunter/qwen3-0.6b-extreme-conditions-lora/tree/main/.\n"
     ]
    }
   ],
   "source": [
    "%pwd\n",
    "# %cd unsloth-qwen-lora\n",
    "# from huggingface_hub import HfApi\n",
    "# cd to the right place, then upload from there\n",
    "!huggingface-cli upload Willgunter/qwen3-0.6b-extreme-conditions-lora . --repo-type model\n",
    "# api = HfApi()\n",
    "# api.upload_folder(\n",
    "#     folder_path=\".\",\n",
    "#     repo_id=\"Willgunter/Extreme-Reasoning-LLM\",\n",
    "#     repo_type=\"model\",\n",
    "#     multi_commits=True,\n",
    "#     multi_commits_verbose=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448,
     "referenced_widgets": [
      "f858797894bd4a0cac2326d9da9b588c",
      "0ff5e66c67da4db5aa4ba1317fa6b738",
      "695356b4c3e747f3a6e5ce0a0f2a8a1e",
      "6676099344d4451eb0945fef5330dbeb",
      "3e9384fad734470f9e082a8608b94af9",
      "baa530a832b34832a96d5c26ce130b15",
      "8b01f05fe53e49ada6b7a063d014bf5c",
      "1656cc6a6ad449c8a44b155a41edb57b",
      "627cd391e0cf4789b8dd4a8619ef1880",
      "8c060a6831e4490287d91fa8339b46ed",
      "e1adc63ae3c84fab9bee6676f52fee2e",
      "38ef6225668e4941b8f9c52b44fb7921",
      "757a9ad63df242ab9a82f887bc40e339",
      "1af677e28d064f76b76afae01ca22f36",
      "394feb3014f0404aacc8336c91092a0e",
      "109f2f4986df40238a6624039d90c4e1",
      "b5fe1aae33684b05a476f83361ac2836",
      "5653a6a266a345f6aa7afa7753755e29",
      "795afc4d1542495785d43eea36953a10",
      "8979ccdb08df4ec49972b1f1c21c00c9",
      "4f6ec84ce5664145920628847640af01",
      "ddf68d642b6748059696d911a52bd3d6",
      "6f6952f396ed412c80182c7a8a20bb7c",
      "f611bcf34c5a422bb05d3eed88cbad83",
      "8f877aace13341e194243a8905e392dd",
      "4bb030af9dc6475a9a8818a5884a611a",
      "6b26b036b06d47a99b26b4fa45afd847",
      "23183630ed4c490989d10dab3ad8516b",
      "cf5e94222def4bb29d74948fa23928e8",
      "851ae2f59e9e48dc908841a4a86a3162",
      "1410e2b327af49c79409448ea407c33b",
      "bd9d55a0d1084f66b286ebaccd3db133",
      "a0f525c972114b13b7d2a5138c6cf0f8",
      "aa6e955056ea4734a9818ee6ee664e84",
      "6ba49fa24fb94a9dbce271fb6646705b",
      "337b9d46148e4aff807a65ed9711d974",
      "1f3ac5a063ba406daa9317b1b7fce19f",
      "25d1803806d943e6af8c4420a697be00",
      "856c2859229c4805b1f0b5fdc5d52c82",
      "30409042e6f94ca7bc6c2b0828e4c4d4",
      "0fcd4b5304334748b43f00f25c49b9cd",
      "3ae7999286c9426aa6fdcd9f4fe1513b",
      "b8121af4c23448f7bd3c2c639ce96286",
      "491b66bd00284502ac7e8997014a7c59",
      "9c64e9dc4a734355be2a2785b731ad31",
      "e62ec967afb54167ab9246d99879ace9",
      "c73ee4e19fd4408996bdef9cd48de335",
      "167a132b5f7046e1a68a7b1b0b74655e",
      "ea6395a0075d40bea56f9687776251ce",
      "e6210bb59e7f4191a5347284a9379d1b",
      "dcff1d7da31f47cba4e68e30dace7d03",
      "828c8ddffef84a8999b6a54a940dff87",
      "26317768f08a4da19ff638dbb6fd6f8a",
      "77a29d7b7d9b407fbdea433b332ef40f",
      "9dad6a89914d482d8dec50dde17693fa",
      "402663dd3356403998d8027a6148cebb",
      "bb40354af17a49efb38d2147fe229536",
      "e1620b46125b4a79bab053d5ba6a460b",
      "c0d05dca5fde4ff49b4ca72b390457ec",
      "ed7bfea8f8874240b402e80351b909ae",
      "8d46816088454f2a85d7ccc923112331",
      "8a29849920c64228ac8c3d56ae646671",
      "1f5cb315b409433898d87dcf2b1b59ba",
      "17fad981500343a3af299350110835bf",
      "a01cbebb72a24440ab6a54eb9d7ad904",
      "8bfdae4bb8dd415ab930bdd9bb0ec979",
      "aaa9e8f1633b4e079f0488e21fbff8fc",
      "2aba542f61914328a820a75323825822",
      "a049c1ec4a884d0cbae526ac89b7f014",
      "0719a1cfede94b55a1ba28985200016f",
      "7345e26939664417a06c6f2b0cfd752c",
      "bf0edb9ccd654fc4ad2e87913d6ae4ca",
      "f9fe5f5747f24f3d9c603ce962d34bf8",
      "e3cdc83526d744e491f2acfbaa7551c5",
      "d46ba69fe1774ccf85887f98b1660c01",
      "f9542acd04754b359d305cdee4da97e5",
      "a9c078359c5049969603475a7c41aa6c",
      "61105698b7e24b27a540f90ec9e6b8eb",
      "b0273a02d0944bd9a734ecab978cab2b",
      "995cf8bb48744f53b5ab8b2ac2022a28",
      "4edf7a2bd1ab48f5824bd5dac3ae2277",
      "f6d9f4f53dc64863bbd65ff5a90eeedc",
      "74af14ac05fa4d568fbebf8ce34fecda",
      "c20b7e1ee8e74617866873dbbe5520f8",
      "47d9d78b00664c50a0eca40b5900e19c",
      "f1324e82f67f4804a4aa7334db511da1",
      "2d914086caaf499fbb58b49adefbdd88",
      "dc1dec119a0c49afa8328d7b4263599e",
      "1a6a83cdb28b4d12a8e73bee2e289bc7",
      "a2ad2000fb864b0198eb51d6fcdfd67f",
      "819ce1954fd2480fbd3c7f8f6271c640",
      "1086c939e66a40bba65b5a9bdf6040ac",
      "d351dbc58226406e9b9a7c0d52525c62",
      "249110bb758a48bab27e5c11cd42d532",
      "1995152b18554285b0bea612c9d9a7d8",
      "e0ae5d477d944de0b4be6e54db6d28f6",
      "07ed789a716146a19d18646dc4c4f831",
      "192b60aa46c84d18be1e64595b0d72ef",
      "5be6cabc0cc44aad8a0c2ccc6c449648"
     ]
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 46007,
     "status": "ok",
     "timestamp": 1769184220141,
     "user": {
      "displayName": "Will Gunter",
      "userId": "04793422759907679153"
     },
     "user_tz": 300
    },
    "id": "dzH7LA3_lS96",
    "outputId": "0d5d2b5e-ed32-4ee9-9a18-4b04e2143af8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f858797894bd4a0cac2326d9da9b588c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ef6225668e4941b8f9c52b44fb7921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6952f396ed412c80182c7a8a20bb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6e955056ea4734a9818ee6ee664e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c64e9dc4a734355be2a2785b731ad31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/323M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402663dd3356403998d8027a6148cebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa9e8f1633b4e079f0488e21fbff8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61105698b7e24b27a540f90ec9e6b8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6a83cdb28b4d12a8e73bee2e289bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./qwen3-0.6b-merged/tokenizer_config.json',\n",
       " './qwen3-0.6b-merged/special_tokens_map.json',\n",
       " './qwen3-0.6b-merged/chat_template.jinja',\n",
       " './qwen3-0.6b-merged/vocab.json',\n",
       " './qwen3-0.6b-merged/merges.txt',\n",
       " './qwen3-0.6b-merged/added_tokens.json',\n",
       " './qwen3-0.6b-merged/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merging to model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "BASE_MODEL = \"Qwen/Qwen3-0.6B-Base\"\n",
    "LORA_MODEL = \"Willgunter/qwen3-0.6b-extreme-conditions-lora\"\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, LORA_MODEL)\n",
    "\n",
    "# Merge adapter weights into base model\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "merged_model.save_pretrained(\"./qwen3-0.6b-merged\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.save_pretrained(\"./qwen3-0.6b-merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1769184376665,
     "user": {
      "displayName": "Will Gunter",
      "userId": "04793422759907679153"
     },
     "user_tz": 300
    },
    "id": "Tn8uGNR_jWHQ",
    "outputId": "c78c72e9-f092-4216-faa4-1d26d5bdd274"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Makefile:6: *** Build system changed:\n",
      " The Makefile build has been replaced by CMake.\n",
      "\n",
      " For build instructions see:\n",
      " https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md\n",
      "\n",
      ".  Stop.\n",
      "Makefile:6: *** Build system changed:\n",
      " The Makefile build has been replaced by CMake.\n",
      "\n",
      " For build instructions see:\n",
      " https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md\n",
      "\n",
      ".  Stop.\n"
     ]
    }
   ],
   "source": [
    "# GGUF stuff - start\n",
    "!make clean\n",
    "# !LLAMA_OPENBLAS=1 make  # Use OpenBLAS\n",
    "# or\n",
    "!LLAMA_BLIS=1 make  # Use BLIS (often faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 9657,
     "status": "ok",
     "timestamp": 1769184533478,
     "user": {
      "displayName": "Will Gunter",
      "userId": "04793422759907679153"
     },
     "user_tz": 300
    },
    "id": "eUGoYwkTlmF4",
    "outputId": "45965c06-6bfd-4010-d63f-4d159da06152"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: qwen3-0.6b-merged\n",
      "INFO:hf-to-gguf:Model architecture: Qwen3ForCausalLM\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model.safetensors'\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.float16 --> F16, shape = {1024, 151936}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float16 --> F16, shape = {1024, 3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float16 --> F16, shape = {2048, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float16 --> F16, shape = {1024, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float16 --> F16, shape = {1024, 1024}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.float16 --> F32, shape = {1024}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 32768\n",
      "INFO:hf-to-gguf:gguf: embedding length = 1024\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 3072\n",
      "INFO:hf-to-gguf:gguf: head count = 16\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 1000000\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:numexpr.utils:NumExpr defaulting to 12 threads.\n",
      "The tokenizer you are loading from '../qwen3-0.6b-merged' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "INFO:gguf.vocab:Adding 151387 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type eos to 151643\n",
      "INFO:gguf.vocab:Setting special token type pad to 151643\n",
      "INFO:gguf.vocab:Setting special token type bos to 151643\n",
      "INFO:gguf.vocab:Setting add_bos_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- messages[0].content + '\\n\\n' }}\n",
      "    {%- endif %}\n",
      "    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n",
      "{%- for message in messages[::-1] %}\n",
      "    {%- set index = (messages|length - 1) - loop.index0 %}\n",
      "    {%- if ns.multi_step_tool and message.role == \"user\" and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n",
      "        {%- set ns.multi_step_tool = false %}\n",
      "        {%- set ns.last_query_index = index %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {%- set content = message.content %}\n",
      "        {%- set reasoning_content = '' %}\n",
      "        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n",
      "            {%- set reasoning_content = message.reasoning_content %}\n",
      "        {%- else %}\n",
      "            {%- if '</think>' in message.content %}\n",
      "                {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n",
      "                {%- set reasoning_content = message.content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n",
      "            {%- endif %}\n",
      "        {%- endif %}\n",
      "        {%- if loop.index0 > ns.last_query_index %}\n",
      "            {%- if loop.last or (not loop.last and reasoning_content) %}\n",
      "                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n",
      "            {%- else %}\n",
      "                {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
      "            {%- endif %}\n",
      "        {%- else %}\n",
      "            {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
      "        {%- endif %}\n",
      "        {%- if message.tool_calls %}\n",
      "            {%- for tool_call in message.tool_calls %}\n",
      "                {%- if (loop.first and content) or (not loop.first) %}\n",
      "                    {{- '\\n' }}\n",
      "                {%- endif %}\n",
      "                {%- if tool_call.function %}\n",
      "                    {%- set tool_call = tool_call.function %}\n",
      "                {%- endif %}\n",
      "                {{- '<tool_call>\\n{\"name\": \"' }}\n",
      "                {{- tool_call.name }}\n",
      "                {{- '\", \"arguments\": ' }}\n",
      "                {%- if tool_call.arguments is string %}\n",
      "                    {{- tool_call.arguments }}\n",
      "                {%- else %}\n",
      "                    {{- tool_call.arguments | tojson }}\n",
      "                {%- endif %}\n",
      "                {{- '}\\n</tool_call>' }}\n",
      "            {%- endfor %}\n",
      "        {%- endif %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "    {%- if enable_thinking is defined and enable_thinking is false %}\n",
      "        {{- '<think>\\n\\n</think>\\n\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:qwen3-0.6b-merged.gguf: n_tensors = 310, total_size = 1.2G\n",
      "Writing: 100% 1.19G/1.19G [00:01<00:00, 1.06Gbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to qwen3-0.6b-merged.gguf\n"
     ]
    }
   ],
   "source": [
    "# more GGUF stuff\n",
    "# !git clone https://github.com/ggerganov/llama.cpp\n",
    "# %cd llama.cpp\n",
    "# %cd unsloth-qwen-lora/llama.cpp\n",
    "# Install requirements\n",
    "# !pip install -r requirements.txt\n",
    "# Convert to GGUF format (FP16 first)\n",
    "!python convert_hf_to_gguf.py ../qwen3-0.6b-merged --outfile qwen3-0.6b-merged.gguf --outtype f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 886625,
     "status": "ok",
     "timestamp": 1769185426233,
     "user": {
      "displayName": "Will Gunter",
      "userId": "04793422759907679153"
     },
     "user_tz": 300
    },
    "id": "3lXIU-qMluOO",
    "outputId": "babe45a4-f15c-41ad-9621-8a5910a01d0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The C compiler identification is GNU 11.4.0\n",
      "-- The CXX compiler identification is GNU 11.4.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n",
      "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
      "-- The ASM compiler identification is GNU\n",
      "-- Found assembler: /usr/bin/cc\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE\n",
      "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "-- GGML_SYSTEM_ARCH: x86\n",
      "-- Including CPU backend\n",
      "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
      "-- Found OpenMP: TRUE (found version \"4.5\")\n",
      "-- x86 detected\n",
      "-- Adding CPU backend variant ggml-cpu: -march=native \n",
      "-- ggml version: 0.9.5\n",
      "-- ggml commit:  cb6caca19\n",
      "-- Found OpenSSL: /usr/lib/x86_64-linux-gnu/libcrypto.so (found version \"3.0.2\")\n",
      "-- Performing Test OPENSSL_VERSION_SUPPORTED\n",
      "-- Performing Test OPENSSL_VERSION_SUPPORTED - Success\n",
      "-- OpenSSL found: 3.0.2\n",
      "-- Generating embedded license file for target: common\n",
      "-- Configuring done (1.4s)\n",
      "-- Generating done (0.3s)\n",
      "-- Build files have been written to: /content/unsloth-qwen-lora/llama.cpp/build\n",
      "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
      "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n",
      "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
      "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
      "[  3%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
      "[  3%] Built target ggml-base\n",
      "[  3%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
      "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
      "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n",
      "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n",
      "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n",
      "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n",
      "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
      "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
      "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
      "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
      "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
      "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
      "[  6%] Built target ggml-cpu\n",
      "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
      "[  6%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
      "[  6%] Built target ggml\n",
      "[  7%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
      "[  7%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
      "[  7%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
      "[  7%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid-iswa.cpp.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/afmoe.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/apertus.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arcee.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arctic.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arwkv7.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/baichuan.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bert.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bitnet.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bloom.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chameleon.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chatglm.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/codeshell.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cogvlm.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/command-r.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dbrx.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deci.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek2.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dots1.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dream.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone4.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone-moe.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gpt2.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gptneox.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grok.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grovemoe.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/internlm2.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jais.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jamba.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/lfm2.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada-moe.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/maincoder.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mamba.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mimo2-iswa.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minicpm3.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/modern-bert.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mpt.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/neo-bert.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo2.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmoe.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openelm.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/orion.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi2.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi3.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo2.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo3.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plm.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2.cpp.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3.cpp.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3next.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/refact.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rnd1.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7.cpp.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/seed-oss.cpp.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smallthinker.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smollm3.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/stablelm.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder2.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-dec.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-enc.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/xverse.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mistral3.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/graph-context-mamba.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
      "[ 38%] Built target llama\n",
      "[ 38%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
      "[ 38%] Built target build_info\n",
      "[ 39%] \u001b[32mBuilding CXX object vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32m\u001b[1mLinking CXX static library libcpp-httplib.a\u001b[0m\n",
      "[ 39%] Built target cpp-httplib\n",
      "[ 39%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-peg-parser.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/debug.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/download.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/peg-parser.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/preset.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/unicode.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/lexer.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/parser.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/runtime.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/value.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/string.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/caps.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/__/license.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
      "[ 45%] Built target common\n",
      "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
      "[ 45%] Built target test-tokenizer-0\n",
      "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
      "[ 45%] Built target test-sampling\n",
      "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
      "[ 45%] Built target test-grammar-parser\n",
      "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
      "[ 46%] Built target test-grammar-integration\n",
      "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
      "[ 46%] Built target test-llama-grammar\n",
      "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
      "[ 46%] Built target test-chat\n",
      "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
      "[ 47%] Built target test-json-schema-to-grammar\n",
      "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\n",
      "[ 47%] Built target test-quantize-stats\n",
      "[ 47%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\n",
      "[ 47%] Built target test-gbnf-validator\n",
      "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
      "[ 48%] Built target test-tokenizer-1-bpe\n",
      "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
      "[ 48%] Built target test-tokenizer-1-spm\n",
      "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\n",
      "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\n",
      "[ 49%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-parser\u001b[0m\n",
      "[ 49%] Built target test-chat-parser\n",
      "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/test-chat-peg-parser.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/peg-parser/simple-tokenize.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/get-model.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-peg-parser\u001b[0m\n",
      "[ 50%] Built target test-chat-peg-parser\n",
      "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
      "[ 51%] Built target test-chat-template\n",
      "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-jinja.dir/test-jinja.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-jinja.dir/get-model.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-jinja\u001b[0m\n",
      "[ 52%] Built target test-jinja\n",
      "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-partial\u001b[0m\n",
      "[ 53%] Built target test-json-partial\n",
      "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
      "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
      "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
      "[ 54%] Built target test-log\n",
      "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/test-peg-parser.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/simple-tokenize.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-basic.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-gbnf-generation.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-json-parser.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-json-serialization.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-unicode.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/get-model.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-peg-parser\u001b[0m\n",
      "[ 56%] Built target test-peg-parser\n",
      "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-regex-partial\u001b[0m\n",
      "[ 57%] Built target test-regex-partial\n",
      "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\n",
      "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\n",
      "[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-thread-safety\u001b[0m\n",
      "[ 58%] Built target test-thread-safety\n",
      "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
      "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
      "[ 58%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
      "[ 58%] Built target test-arg-parser\n",
      "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/get-model.cpp.o\u001b[0m\n",
      "[ 59%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-opt\u001b[0m\n",
      "[ 59%] Built target test-opt\n",
      "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
      "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
      "[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
      "[ 60%] Built target test-gguf\n",
      "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
      "[ 61%] Built target test-backend-ops\n",
      "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
      "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
      "[ 62%] Built target test-model-load-cancel\n",
      "[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
      "[ 63%] Built target test-autorelease\n",
      "[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-sampler.dir/test-backend-sampler.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-sampler.dir/get-model.cpp.o\u001b[0m\n",
      "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-sampler\u001b[0m\n",
      "[ 63%] Built target test-backend-sampler\n",
      "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-state-restore-fragmented.dir/test-state-restore-fragmented.cpp.o\u001b[0m\n",
      "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-state-restore-fragmented.dir/get-model.cpp.o\u001b[0m\n",
      "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-state-restore-fragmented\u001b[0m\n",
      "[ 64%] Built target test-state-restore-fragmented\n",
      "[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
      "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
      "[ 65%] Built target test-barrier\n",
      "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
      "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
      "[ 66%] Built target test-quantize-fns\n",
      "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
      "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
      "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
      "[ 67%] Built target test-quantize-perf\n",
      "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
      "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
      "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
      "[ 68%] Built target test-rope\n",
      "[ 69%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n",
      "[ 69%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\u001b[0m\n",
      "[ 69%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\u001b[0m\n",
      "[ 69%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/cogvlm.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/conformer.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/glm4v.cpp.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/internvl.cpp.o\u001b[0m\n",
      "[ 71%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/kimivl.cpp.o\u001b[0m\n",
      "[ 71%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llama4.cpp.o\u001b[0m\n",
      "[ 71%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llava.cpp.o\u001b[0m\n",
      "[ 71%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/minicpmv.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/pixtral.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen2vl.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen3vl.cpp.o\u001b[0m\n",
      "[ 72%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/siglip.cpp.o\u001b[0m\n",
      "[ 73%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/whisper-enc.cpp.o\u001b[0m\n",
      "[ 73%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/mobilenetv5.cpp.o\u001b[0m\n",
      "[ 73%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/youtuvl.cpp.o\u001b[0m\n",
      "[ 73%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\n",
      "[ 73%] Built target mtmd\n",
      "[ 73%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\n",
      "[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\n",
      "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\n",
      "[ 73%] Built target test-mtmd-c-api\n",
      "[ 73%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
      "[ 74%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
      "[ 74%] Built target test-c\n",
      "[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/test-alloc.cpp.o\u001b[0m\n",
      "[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/get-model.cpp.o\u001b[0m\n",
      "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-alloc\u001b[0m\n",
      "[ 75%] Built target test-alloc\n",
      "[ 75%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
      "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
      "[ 75%] Built target llama-batched\n",
      "[ 76%] \u001b[32mBuilding CXX object examples/debug/CMakeFiles/llama-debug.dir/debug.cpp.o\u001b[0m\n",
      "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-debug\u001b[0m\n",
      "[ 76%] Built target llama-debug\n",
      "[ 77%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
      "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
      "[ 77%] Built target llama-embedding\n",
      "[ 77%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
      "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
      "[ 77%] Built target llama-eval-callback\n",
      "[ 77%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
      "[ 77%] Built target sha256\n",
      "[ 78%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
      "[ 78%] Built target xxhash\n",
      "[ 79%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
      "[ 79%] Built target sha1\n",
      "[ 79%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
      "[ 80%] Built target llama-gguf-hash\n",
      "[ 80%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
      "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
      "[ 80%] Built target llama-gguf\n",
      "[ 80%] \u001b[32mBuilding CXX object examples/idle/CMakeFiles/llama-idle.dir/idle.cpp.o\u001b[0m\n",
      "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-idle\u001b[0m\n",
      "[ 81%] Built target llama-idle\n",
      "[ 82%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
      "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
      "[ 82%] Built target llama-lookahead\n",
      "[ 82%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
      "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
      "[ 82%] Built target llama-lookup\n",
      "[ 83%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
      "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
      "[ 83%] Built target llama-lookup-create\n",
      "[ 83%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
      "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
      "[ 83%] Built target llama-lookup-merge\n",
      "[ 84%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
      "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
      "[ 84%] Built target llama-lookup-stats\n",
      "[ 84%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
      "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
      "[ 84%] Built target llama-parallel\n",
      "[ 85%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
      "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
      "[ 85%] Built target llama-passkey\n",
      "[ 85%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
      "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
      "[ 85%] Built target llama-retrieval\n",
      "[ 85%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
      "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
      "[ 86%] Built target llama-save-load-state\n",
      "[ 86%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
      "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
      "[ 87%] Built target llama-simple\n",
      "[ 87%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
      "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
      "[ 87%] Built target llama-simple-chat\n",
      "[ 87%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
      "[ 88%] Built target llama-speculative\n",
      "[ 88%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
      "[ 88%] Built target llama-speculative-simple\n",
      "[ 88%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
      "[ 89%] Built target llama-gen-docs\n",
      "[ 89%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-finetune\u001b[0m\n",
      "[ 89%] Built target llama-finetune\n",
      "[ 89%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\n",
      "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\n",
      "[ 89%] Built target llama-diffusion-cli\n",
      "[ 90%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
      "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
      "[ 90%] Built target llama-convert-llama2c-to-ggml\n",
      "[ 90%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
      "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
      "[ 90%] Built target llama-vdot\n",
      "[ 91%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
      "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
      "[ 91%] Built target llama-q8dot\n",
      "[ 92%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
      "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
      "[ 92%] Built target llama-batched-bench\n",
      "[ 92%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
      "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
      "[ 92%] Built target llama-gguf-split\n",
      "[ 92%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
      "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
      "[ 92%] Built target llama-imatrix\n",
      "[ 92%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
      "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
      "[ 92%] Built target llama-bench\n",
      "[ 92%] \u001b[32mBuilding CXX object tools/completion/CMakeFiles/llama-completion.dir/completion.cpp.o\u001b[0m\n",
      "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-completion\u001b[0m\n",
      "[ 92%] Built target llama-completion\n",
      "[ 92%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
      "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
      "[ 92%] Built target llama-perplexity\n",
      "[ 92%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
      "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
      "[ 92%] Built target llama-quantize\n",
      "[ 92%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-task.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-queue.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-common.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-context.cpp.o\u001b[0m\n",
      "[ 93%] \u001b[32m\u001b[1mLinking CXX static library libserver-context.a\u001b[0m\n",
      "[ 93%] Built target server-context\n",
      "[ 94%] \u001b[32mBuilding CXX object tools/cli/CMakeFiles/llama-cli.dir/cli.cpp.o\u001b[0m\n",
      "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
      "[ 94%] Built target llama-cli\n",
      "[ 94%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
      "[ 94%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
      "[ 94%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-http.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-models.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
      "[ 95%] Built target llama-server\n",
      "[ 95%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
      "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
      "[ 95%] Built target llama-tokenize\n",
      "[ 96%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
      "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
      "[ 96%] Built target llama-tts\n",
      "[ 96%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
      "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
      "[ 96%] Built target llama-llava-cli\n",
      "[ 96%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
      "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
      "[ 96%] Built target llama-gemma3-cli\n",
      "[ 96%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
      "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
      "[ 96%] Built target llama-minicpmv-cli\n",
      "[ 96%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
      "[ 97%] Built target llama-qwen2vl-cli\n",
      "[ 98%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n",
      "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n",
      "[ 98%] Built target llama-mtmd-cli\n",
      "[ 98%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
      "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
      "[ 98%] Built target llama-cvector-generator\n",
      "[ 98%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
      "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
      "[ 99%] Built target llama-export-lora\n",
      "[ 99%] \u001b[32mBuilding CXX object tools/fit-params/CMakeFiles/llama-fit-params.dir/fit-params.cpp.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-fit-params\u001b[0m\n",
      "[100%] Built target llama-fit-params\n",
      "main: build = 7818 (cb6caca19)\n",
      "main: built with GNU 11.4.0 for Linux x86_64\n",
      "main: quantizing 'qwen3-0.6b-merged.gguf' to 'qwen3-0.6b-merged-q4_k_m.gguf' as Q4_K_M\n",
      "llama_model_loader: direct I/O is enabled, disabling mmap\n",
      "llama_model_loader: loaded meta data with 28 key-value pairs and 310 tensors from qwen3-0.6b-merged.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen3\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen3 0.6b Merged\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = merged\n",
      "llama_model_loader: - kv   4:                           general.basename str              = qwen3\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 0.6B\n",
      "llama_model_loader: - kv   6:                          qwen3.block_count u32              = 28\n",
      "llama_model_loader: - kv   7:                       qwen3.context_length u32              = 32768\n",
      "llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 1024\n",
      "llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 3072\n",
      "llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  16:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ä  Ä \", \"Ä Ä  Ä Ä \", \"i n\", \"Ä  t\",...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - type  f32:  113 tensors\n",
      "llama_model_loader: - type  f16:  197 tensors\n",
      "[   1/ 310]                   output_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[   2/ 310]                    token_embd.weight - [ 1024, 151936,     1,     1], type =    f16, converting to q6_K .. size =   296.75 MiB ->   121.71 MiB\n",
      "[   3/ 310]                  blk.0.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[   4/ 310]             blk.0.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[   5/ 310]               blk.0.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[   6/ 310]             blk.0.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[   7/ 310]                  blk.0.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[   8/ 310]             blk.0.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[   9/ 310]                  blk.0.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[  10/ 310]                blk.0.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  11/ 310]                blk.0.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  12/ 310]                blk.0.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[  13/ 310]                  blk.0.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  14/ 310]                  blk.1.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  15/ 310]             blk.1.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  16/ 310]               blk.1.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[  17/ 310]             blk.1.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[  18/ 310]                  blk.1.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[  19/ 310]             blk.1.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  20/ 310]                  blk.1.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[  21/ 310]                blk.1.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  22/ 310]                blk.1.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  23/ 310]                blk.1.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[  24/ 310]                  blk.1.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  25/ 310]                  blk.2.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  26/ 310]             blk.2.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  27/ 310]               blk.2.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[  28/ 310]             blk.2.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[  29/ 310]                  blk.2.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[  30/ 310]             blk.2.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  31/ 310]                  blk.2.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[  32/ 310]                blk.2.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  33/ 310]                blk.2.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  34/ 310]                blk.2.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[  35/ 310]                  blk.2.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  36/ 310]                  blk.3.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  37/ 310]             blk.3.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  38/ 310]               blk.3.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[  39/ 310]             blk.3.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[  40/ 310]                  blk.3.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[  41/ 310]             blk.3.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  42/ 310]                  blk.3.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  43/ 310]                blk.3.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  44/ 310]                blk.3.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  45/ 310]                blk.3.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[  46/ 310]                  blk.3.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  47/ 310]                  blk.4.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  48/ 310]             blk.4.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  49/ 310]               blk.4.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[  50/ 310]             blk.4.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[  51/ 310]                  blk.4.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[  52/ 310]             blk.4.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  53/ 310]                  blk.4.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  54/ 310]                blk.4.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  55/ 310]                blk.4.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  56/ 310]                blk.4.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[  57/ 310]                  blk.4.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  58/ 310]                  blk.5.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  59/ 310]             blk.5.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  60/ 310]               blk.5.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[  61/ 310]             blk.5.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[  62/ 310]                  blk.5.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[  63/ 310]             blk.5.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  64/ 310]                  blk.5.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[  65/ 310]                blk.5.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  66/ 310]                blk.5.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  67/ 310]                blk.5.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[  68/ 310]                  blk.5.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  69/ 310]                  blk.6.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  70/ 310]             blk.6.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  71/ 310]               blk.6.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[  72/ 310]             blk.6.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[  73/ 310]                  blk.6.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[  74/ 310]             blk.6.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  75/ 310]                  blk.6.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  76/ 310]                blk.6.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  77/ 310]                blk.6.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  78/ 310]                blk.6.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[  79/ 310]                  blk.6.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  80/ 310]                  blk.7.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  81/ 310]             blk.7.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  82/ 310]               blk.7.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[  83/ 310]             blk.7.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[  84/ 310]                  blk.7.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[  85/ 310]             blk.7.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  86/ 310]                  blk.7.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  87/ 310]                blk.7.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  88/ 310]                blk.7.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  89/ 310]                blk.7.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[  90/ 310]                  blk.7.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  91/ 310]                  blk.8.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[  92/ 310]             blk.8.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  93/ 310]               blk.8.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[  94/ 310]             blk.8.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[  95/ 310]                  blk.8.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[  96/ 310]             blk.8.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[  97/ 310]                  blk.8.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[  98/ 310]                blk.8.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  99/ 310]                blk.8.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 100/ 310]                blk.8.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 101/ 310]                  blk.8.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 102/ 310]                  blk.9.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 103/ 310]             blk.9.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 104/ 310]               blk.9.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 105/ 310]             blk.9.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 106/ 310]                  blk.9.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 107/ 310]             blk.9.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 108/ 310]                  blk.9.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 109/ 310]                blk.9.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 110/ 310]                blk.9.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 111/ 310]                blk.9.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 112/ 310]                  blk.9.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 113/ 310]                 blk.10.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 114/ 310]            blk.10.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 115/ 310]              blk.10.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 116/ 310]            blk.10.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 117/ 310]                 blk.10.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 118/ 310]            blk.10.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 119/ 310]                 blk.10.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 120/ 310]               blk.10.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 121/ 310]               blk.10.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 122/ 310]               blk.10.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 123/ 310]                 blk.10.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 124/ 310]                 blk.11.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 125/ 310]            blk.11.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 126/ 310]              blk.11.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 127/ 310]            blk.11.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 128/ 310]                 blk.11.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 129/ 310]            blk.11.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 130/ 310]                 blk.11.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[ 131/ 310]               blk.11.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 132/ 310]               blk.11.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 133/ 310]               blk.11.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 134/ 310]                 blk.11.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 135/ 310]                 blk.12.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 136/ 310]            blk.12.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 137/ 310]              blk.12.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 138/ 310]            blk.12.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 139/ 310]                 blk.12.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 140/ 310]            blk.12.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 141/ 310]                 blk.12.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 142/ 310]               blk.12.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 143/ 310]               blk.12.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 144/ 310]               blk.12.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 145/ 310]                 blk.12.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 146/ 310]                 blk.13.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 147/ 310]            blk.13.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 148/ 310]              blk.13.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 149/ 310]            blk.13.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 150/ 310]                 blk.13.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 151/ 310]            blk.13.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 152/ 310]                 blk.13.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 153/ 310]               blk.13.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 154/ 310]               blk.13.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 155/ 310]               blk.13.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 156/ 310]                 blk.13.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 157/ 310]                 blk.14.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 158/ 310]            blk.14.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 159/ 310]              blk.14.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 160/ 310]            blk.14.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 161/ 310]                 blk.14.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 162/ 310]            blk.14.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 163/ 310]                 blk.14.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[ 164/ 310]               blk.14.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 165/ 310]               blk.14.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 166/ 310]               blk.14.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 167/ 310]                 blk.14.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 168/ 310]                 blk.15.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 169/ 310]            blk.15.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 170/ 310]              blk.15.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 171/ 310]            blk.15.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 172/ 310]                 blk.15.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 173/ 310]            blk.15.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 174/ 310]                 blk.15.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 175/ 310]               blk.15.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 176/ 310]               blk.15.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 177/ 310]               blk.15.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 178/ 310]                 blk.15.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 179/ 310]                 blk.16.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 180/ 310]            blk.16.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 181/ 310]              blk.16.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 182/ 310]            blk.16.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 183/ 310]                 blk.16.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 184/ 310]            blk.16.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 185/ 310]                 blk.16.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 186/ 310]               blk.16.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 187/ 310]               blk.16.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 188/ 310]               blk.16.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 189/ 310]                 blk.16.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 190/ 310]                 blk.17.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 191/ 310]            blk.17.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 192/ 310]              blk.17.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 193/ 310]            blk.17.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 194/ 310]                 blk.17.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 195/ 310]            blk.17.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 196/ 310]                 blk.17.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[ 197/ 310]               blk.17.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 198/ 310]               blk.17.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 199/ 310]               blk.17.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 200/ 310]                 blk.17.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 201/ 310]                 blk.18.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 202/ 310]            blk.18.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 203/ 310]              blk.18.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 204/ 310]            blk.18.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 205/ 310]                 blk.18.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 206/ 310]            blk.18.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 207/ 310]                 blk.18.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 208/ 310]               blk.18.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 209/ 310]               blk.18.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 210/ 310]               blk.18.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 211/ 310]                 blk.18.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 212/ 310]                 blk.19.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 213/ 310]            blk.19.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 214/ 310]              blk.19.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 215/ 310]            blk.19.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 216/ 310]                 blk.19.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 217/ 310]            blk.19.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 218/ 310]                 blk.19.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 219/ 310]               blk.19.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 220/ 310]               blk.19.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 221/ 310]               blk.19.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 222/ 310]                 blk.19.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 223/ 310]                 blk.20.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 224/ 310]            blk.20.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 225/ 310]              blk.20.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 226/ 310]            blk.20.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 227/ 310]                 blk.20.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 228/ 310]            blk.20.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 229/ 310]                 blk.20.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[ 230/ 310]               blk.20.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 231/ 310]               blk.20.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 232/ 310]               blk.20.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 233/ 310]                 blk.20.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 234/ 310]                 blk.21.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 235/ 310]            blk.21.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 236/ 310]              blk.21.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 237/ 310]            blk.21.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 238/ 310]                 blk.21.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 239/ 310]            blk.21.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 240/ 310]                 blk.21.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 241/ 310]               blk.21.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 242/ 310]               blk.21.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 243/ 310]               blk.21.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 244/ 310]                 blk.21.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 245/ 310]                 blk.22.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 246/ 310]            blk.22.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 247/ 310]              blk.22.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 248/ 310]            blk.22.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 249/ 310]                 blk.22.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 250/ 310]            blk.22.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 251/ 310]                 blk.22.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 252/ 310]               blk.22.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 253/ 310]               blk.22.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 254/ 310]               blk.22.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 255/ 310]                 blk.22.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 256/ 310]                 blk.23.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 257/ 310]            blk.23.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 258/ 310]              blk.23.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 259/ 310]            blk.23.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 260/ 310]                 blk.23.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 261/ 310]            blk.23.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 262/ 310]                 blk.23.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[ 263/ 310]               blk.23.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 264/ 310]               blk.23.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 265/ 310]               blk.23.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 266/ 310]                 blk.23.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 267/ 310]                 blk.24.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 268/ 310]            blk.24.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 269/ 310]              blk.24.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 270/ 310]            blk.24.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 271/ 310]                 blk.24.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 272/ 310]            blk.24.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 273/ 310]                 blk.24.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[ 274/ 310]               blk.24.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 275/ 310]               blk.24.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 276/ 310]               blk.24.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 277/ 310]                 blk.24.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 278/ 310]                 blk.25.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 279/ 310]            blk.25.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 280/ 310]              blk.25.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 281/ 310]            blk.25.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 282/ 310]                 blk.25.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 283/ 310]            blk.25.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 284/ 310]                 blk.25.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[ 285/ 310]               blk.25.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 286/ 310]               blk.25.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 287/ 310]               blk.25.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 288/ 310]                 blk.25.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 289/ 310]                 blk.26.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 290/ 310]            blk.26.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 291/ 310]              blk.26.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 292/ 310]            blk.26.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 293/ 310]                 blk.26.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 294/ 310]            blk.26.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 295/ 310]                 blk.26.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[ 296/ 310]               blk.26.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 297/ 310]               blk.26.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 298/ 310]               blk.26.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 299/ 310]                 blk.26.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 300/ 310]                 blk.27.attn_k.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
      "[ 301/ 310]            blk.27.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 302/ 310]              blk.27.attn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 303/ 310]            blk.27.attn_output.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 304/ 310]                 blk.27.attn_q.weight - [ 1024,  2048,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n",
      "[ 305/ 310]            blk.27.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MiB\n",
      "[ 306/ 310]                 blk.27.attn_v.weight - [ 1024,  1024,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
      "[ 307/ 310]               blk.27.ffn_down.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 308/ 310]               blk.27.ffn_gate.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 309/ 310]               blk.27.ffn_norm.weight - [ 1024,     1,     1,     1], type =    f32, size =    0.004 MiB\n",
      "[ 310/ 310]                 blk.27.ffn_up.weight - [ 1024,  3072,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "llama_model_quantize_impl: model size  =  1137.00 MiB\n",
      "llama_model_quantize_impl: quant size  =   372.65 MiB\n",
      "\n",
      "main: quantize time = 14326.36 ms\n",
      "main:    total time = 14326.36 ms\n"
     ]
    }
   ],
   "source": [
    "# Build the quantize tool (more GGUF stuff)\n",
    "# !cmake quantize\n",
    "# !ls -lh *.gguf\n",
    "# !find . -name \"*q4_k_m.gguf\" -type f\n",
    "!cmake -B build && cmake --build build\n",
    "# !./build/bin/llama-quantize\n",
    "!./build/bin/llama-quantize qwen3-0.6b-merged.gguf qwen3-0.6b-merged-q4_k_m.gguf Q4_K_M\n",
    "# %pwd\n",
    "# # Quantize to Q4_K_M (good balance of size/quality)\n",
    "# !./llama-quantize qwen3-4b-merged.gguf qwen3-4b-merged-q4_k_m.gguf Q4_K_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HCUSSzol3XZ"
   },
   "outputs": [],
   "source": [
    "# upload this artifact --> qwen3-8b-merged-q4_k_m.gguf to HF, then change gradio code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1265,
     "status": "ok",
     "timestamp": 1769185440014,
     "user": {
      "displayName": "Will Gunter",
      "userId": "04793422759907679153"
     },
     "user_tz": 300
    },
    "id": "KHLSt59tmnXr",
    "outputId": "3774fa46-f338-45e0-d915-1d4de120ce75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe --type argument is deprecated and will be removed in a future version. Use --repo-type instead.\u001b[0m\n",
      "\u001b[33mâš ï¸  Warning: 'huggingface-cli repo' is deprecated. Use 'hf repo' instead.\u001b[0m\n",
      "Successfully created \u001b[1mWillgunter/qwen3-0.6b-extreme-conditions-gguf\u001b[0m on the Hub.\n",
      "Your repo is now available at \u001b[1mhttps://huggingface.co/Willgunter/qwen3-0.6b-extreme-conditions-gguf\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli repo create qwen3-0.6b-extreme-conditions-gguf --type model\n",
    "# %pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 19663,
     "status": "ok",
     "timestamp": 1769185476884,
     "user": {
      "displayName": "Will Gunter",
      "userId": "04793422759907679153"
     },
     "user_tz": 300
    },
    "id": "IdrjIqjomp8f",
    "outputId": "99508337-e380-474b-b03a-674916fd061f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mâš ï¸  Warning: 'huggingface-cli upload' is deprecated. Use 'hf upload' instead.\u001b[0m\n",
      "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n",
      "New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n",
      "\n",
      "  ...3-0.6b-merged-q4_k_m.gguf:   8% 33.7M/397M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :   8% 33.7M/397M [00:02<00:25, 14.0MB/s, 15.3MB/s  ]\n",
      "\n",
      "Processing Files (0 / 1)      :  17% 67.2M/397M [00:02<00:10, 30.3MB/s, 28.0MB/s  ]\n",
      "\n",
      "Processing Files (0 / 1)      :  23% 91.4M/397M [00:02<00:07, 42.2MB/s, 35.2MB/s  ]\n",
      "\n",
      "  ...3-0.6b-merged-q4_k_m.gguf:  23% 91.4M/397M [00:00<00:03, 96.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  23% 92.0M/397M [00:03<00:09, 31.1MB/s, 30.7MB/s  ]\n",
      "New Data Upload               :   1% 550k/67.0M [00:03<06:27, 171kB/s,  183kB/s  ]\u001b[A\n",
      "\n",
      "  ...3-0.6b-merged-q4_k_m.gguf:  23% 92.0M/397M [00:00<00:05, 58.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  ...3-0.6b-merged-q4_k_m.gguf:  23% 92.0M/397M [00:01<00:06, 48.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  ...3-0.6b-merged-q4_k_m.gguf:  23% 92.0M/397M [00:01<00:07, 41.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  23% 92.5M/397M [00:04<00:17, 17.8MB/s, 24.4MB/s  ]\n",
      "New Data Upload               :   1% 1.10M/134M [00:04<07:13, 307kB/s,  289kB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  24% 94.2M/397M [00:04<00:18, 16.4MB/s, 23.5MB/s  ]\n",
      "New Data Upload               :   1% 2.72M/201M [00:04<03:23, 977kB/s,  681kB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  24% 96.9M/397M [00:04<00:18, 16.0MB/s, 23.1MB/s  ]\n",
      "New Data Upload               :   3% 5.47M/201M [00:04<01:22, 2.37MB/s, 1.30MB/s  ]\u001b[A\n",
      "\n",
      "  ...3-0.6b-merged-q4_k_m.gguf:  24% 96.9M/397M [00:02<00:10, 28.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  25% 98.5M/397M [00:04<00:24, 12.2MB/s, 21.4MB/s  ]\n",
      "New Data Upload               :   4% 7.10M/201M [00:04<01:10, 2.77MB/s, 1.54MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  26% 102M/397M [00:05<00:21, 13.4MB/s, 21.3MB/s  ] \n",
      "New Data Upload               :   5% 10.8M/201M [00:05<00:37, 5.09MB/s, 2.26MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  27% 106M/397M [00:05<00:20, 14.0MB/s, 21.1MB/s  ]\n",
      "New Data Upload               :   7% 14.1M/201M [00:05<00:26, 6.98MB/s, 2.81MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  27% 109M/397M [00:05<00:19, 14.6MB/s, 20.9MB/s  ]\n",
      "New Data Upload               :   6% 17.4M/268M [00:05<00:28, 8.85MB/s, 3.34MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  29% 113M/397M [00:05<00:17, 16.3MB/s, 21.0MB/s  ]\n",
      "New Data Upload               :   7% 21.7M/305M [00:05<00:24, 11.7MB/s, 4.02MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  31% 121M/397M [00:05<00:12, 22.5MB/s, 21.6MB/s  ]\n",
      "New Data Upload               :  10% 29.7M/305M [00:05<00:14, 18.5MB/s, 5.31MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  33% 129M/397M [00:06<00:09, 27.3MB/s, 22.3MB/s  ]\n",
      "New Data Upload               :  12% 37.8M/305M [00:06<00:11, 24.0MB/s, 6.51MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  34% 136M/397M [00:06<00:08, 29.4MB/s, 22.7MB/s  ]\n",
      "New Data Upload               :  15% 44.7M/305M [00:06<00:09, 26.9MB/s, 7.45MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  36% 144M/397M [00:06<00:07, 32.4MB/s, 23.3MB/s  ]\n",
      "New Data Upload               :  17% 52.7M/305M [00:06<00:08, 30.5MB/s, 8.51MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  38% 150M/397M [00:06<00:07, 32.2MB/s, 23.5MB/s  ]\n",
      "New Data Upload               :  19% 59.0M/305M [00:06<00:07, 30.8MB/s, 9.22MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  40% 161M/397M [00:06<00:06, 37.5MB/s, 24.3MB/s  ]\n",
      "New Data Upload               :  23% 69.1M/305M [00:06<00:06, 36.5MB/s, 10.5MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  43% 172M/397M [00:07<00:05, 43.7MB/s, 25.3MB/s  ]\n",
      "New Data Upload               :  27% 80.8M/305M [00:07<00:05, 42.8MB/s, 11.9MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  46% 182M/397M [00:07<00:04, 45.7MB/s, 26.1MB/s  ]\n",
      "New Data Upload               :  30% 90.9M/305M [00:07<00:04, 45.1MB/s, 13.0MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  49% 194M/397M [00:07<00:04, 48.8MB/s, 26.9MB/s  ]\n",
      "New Data Upload               :  33% 102M/305M [00:07<00:04, 48.3MB/s, 14.2MB/s  ] \u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  52% 206M/397M [00:07<00:03, 53.2MB/s, 27.9MB/s  ]\n",
      "New Data Upload               :  38% 115M/305M [00:07<00:03, 52.9MB/s, 15.5MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  55% 216M/397M [00:07<00:03, 52.4MB/s, 28.5MB/s  ]\n",
      "New Data Upload               :  41% 125M/305M [00:07<00:03, 52.2MB/s, 16.4MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  57% 226M/397M [00:08<00:03, 51.0MB/s, 29.0MB/s  ]\n",
      "New Data Upload               :  44% 134M/305M [00:08<00:03, 50.8MB/s, 17.2MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  61% 240M/397M [00:08<00:02, 57.0MB/s, 30.0MB/s  ]\n",
      "New Data Upload               :  49% 149M/305M [00:08<00:02, 56.9MB/s, 18.6MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  64% 256M/397M [00:08<00:02, 62.9MB/s, 31.2MB/s  ]\n",
      "New Data Upload               :  54% 164M/305M [00:08<00:02, 62.9MB/s, 20.0MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  68% 271M/397M [00:08<00:01, 67.1MB/s, 32.2MB/s  ]\n",
      "New Data Upload               :  59% 179M/305M [00:08<00:01, 67.0MB/s, 21.4MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  72% 285M/397M [00:08<00:01, 68.4MB/s, 33.2MB/s  ]\n",
      "New Data Upload               :  64% 194M/305M [00:08<00:01, 68.4MB/s, 22.5MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  75% 296M/397M [00:09<00:01, 63.9MB/s, 33.6MB/s  ]\n",
      "New Data Upload               :  67% 204M/305M [00:09<00:01, 63.8MB/s, 23.2MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  77% 307M/397M [00:09<00:01, 61.4MB/s, 34.1MB/s  ]\n",
      "New Data Upload               :  71% 215M/305M [00:09<00:01, 61.3MB/s, 23.9MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  80% 317M/397M [00:09<00:01, 58.6MB/s, 34.5MB/s  ]\n",
      "New Data Upload               :  74% 226M/305M [00:09<00:01, 58.6MB/s, 24.6MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  83% 328M/397M [00:09<00:01, 56.8MB/s, 34.9MB/s  ]\n",
      "New Data Upload               :  78% 236M/305M [00:09<00:01, 56.8MB/s, 25.2MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  86% 340M/397M [00:09<00:00, 58.6MB/s, 35.5MB/s  ]\n",
      "New Data Upload               :  82% 249M/305M [00:09<00:00, 58.6MB/s, 25.9MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  89% 353M/397M [00:10<00:00, 59.1MB/s, 36.0MB/s  ]\n",
      "New Data Upload               :  86% 261M/305M [00:10<00:00, 59.1MB/s, 26.6MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  91% 361M/397M [00:10<00:00, 54.8MB/s, 36.1MB/s  ]\n",
      "New Data Upload               :  89% 270M/305M [00:10<00:00, 54.8MB/s, 27.0MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  93% 367M/397M [00:10<00:00, 47.0MB/s, 36.0MB/s  ]\n",
      "New Data Upload               :  90% 276M/305M [00:10<00:00, 47.0MB/s, 27.0MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  93% 370M/397M [00:10<00:00, 37.6MB/s, 36.3MB/s  ]\n",
      "New Data Upload               :  91% 279M/305M [00:10<00:00, 37.6MB/s, 27.3MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  95% 377M/397M [00:10<00:00, 35.8MB/s, 36.9MB/s  ]\n",
      "New Data Upload               :  94% 285M/305M [00:10<00:00, 35.8MB/s, 28.0MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  96% 382M/397M [00:11<00:00, 32.7MB/s, 37.4MB/s  ]\n",
      "New Data Upload               :  95% 290M/305M [00:11<00:00, 32.7MB/s, 28.5MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  97% 385M/397M [00:11<00:00, 28.3MB/s, 37.8MB/s  ]\n",
      "New Data Upload               :  96% 294M/305M [00:11<00:00, 28.3MB/s, 28.8MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  98% 389M/397M [00:11<00:00, 24.5MB/s, 38.1MB/s  ]\n",
      "New Data Upload               :  97% 297M/305M [00:11<00:00, 24.5MB/s, 29.1MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  99% 392M/397M [00:11<00:00, 21.9MB/s, 38.4MB/s  ]\n",
      "New Data Upload               :  98% 300M/305M [00:11<00:00, 21.9MB/s, 29.4MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      :  99% 394M/397M [00:11<00:00, 19.3MB/s, 38.7MB/s  ]\n",
      "New Data Upload               :  99% 303M/305M [00:11<00:00, 19.3MB/s, 29.7MB/s  ]\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      : 100% 396M/397M [00:12<00:00, 15.8MB/s, 38.8MB/s  ]\n",
      "New Data Upload               : 100% 304M/305M [00:12<00:00, 15.8MB/s, 29.8MB/s  ]\u001b[A\n",
      "\n",
      "  ...3-0.6b-merged-q4_k_m.gguf: 100% 396M/397M [00:09<00:00, 37.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  ...3-0.6b-merged-q4_k_m.gguf: 100% 396M/397M [00:10<00:00, 36.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  ...3-0.6b-merged-q4_k_m.gguf: 100% 396M/397M [00:10<00:00, 35.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  ...3-0.6b-merged-q4_k_m.gguf: 100% 396M/397M [00:10<00:00, 34.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  ...3-0.6b-merged-q4_k_m.gguf: 100% 396M/397M [00:10<00:00, 34.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      : 100% 396M/397M [00:13<00:00, 4.71MB/s, 29.9MB/s  ]\n",
      "New Data Upload               : 100% 305M/305M [00:13<00:00, 4.71MB/s, 29.9MB/s  ]\u001b[A\n",
      "\n",
      "  ...3-0.6b-merged-q4_k_m.gguf: 100% 396M/397M [00:10<00:00, 33.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  ...3-0.6b-merged-q4_k_m.gguf: 100% 396M/397M [00:11<00:00, 32.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  ...3-0.6b-merged-q4_k_m.gguf: 100% 396M/397M [00:11<00:00, 32.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Processing Files (0 / 1)      : 100% 396M/397M [00:13<00:00, 28.7MB/s, 30.4MB/s  ]\n",
      "New Data Upload               : 100% 305M/305M [00:13<00:00, 22.1MB/s, 30.4MB/s  ]\n",
      "  ...3-0.6b-merged-q4_k_m.gguf: 100% 396M/397M [00:11<00:00, 31.8MB/s]\n",
      "https://huggingface.co/Willgunter/qwen3-0.6b-extreme-conditions-gguf/blob/main/qwen3-0.6b-merged-q4_k_m.gguf\n"
     ]
    }
   ],
   "source": [
    "# %cd qwen3-4b-merged\n",
    "# %ls\n",
    "# from huggingface_hub import HfApi\n",
    "# cd to the right place, then upload from there\n",
    "!huggingface-cli upload Willgunter/qwen3-0.6b-extreme-conditions-gguf qwen3-0.6b-merged-q4_k_m.gguf --repo-type model\n",
    "# api = HfApi()\n",
    "# api.upload_folder(\n",
    "#     folder_path=\".\",\n",
    "#     repo_id=\"Willgunter/Extreme-Reasoning-LLM\",\n",
    "#     repo_type=\"model\",\n",
    "#     multi_commits=True,\n",
    "#     multi_commits_verbose=True\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyODptp0kHKd7ySK0mhqRqPD",
   "gpuType": "L4",
   "mount_file_id": "1oa-9B9IFNI3X1JYcwtzufN2ONtlhil9K",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0719a1cfede94b55a1ba28985200016f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9542acd04754b359d305cdee4da97e5",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a9c078359c5049969603475a7c41aa6c",
      "value": "â€‡2.78M/?â€‡[00:00&lt;00:00,â€‡8.49MB/s]"
     }
    },
    "07ed789a716146a19d18646dc4c4f831": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0fcd4b5304334748b43f00f25c49b9cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "0ff5e66c67da4db5aa4ba1317fa6b738": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_baa530a832b34832a96d5c26ce130b15",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8b01f05fe53e49ada6b7a063d014bf5c",
      "value": "config.json:â€‡100%"
     }
    },
    "1086c939e66a40bba65b5a9bdf6040ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_192b60aa46c84d18be1e64595b0d72ef",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5be6cabc0cc44aad8a0c2ccc6c449648",
      "value": "â€‡7.03M/?â€‡[00:00&lt;00:00,â€‡14.9MB/s]"
     }
    },
    "109f2f4986df40238a6624039d90c4e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1410e2b327af49c79409448ea407c33b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1656cc6a6ad449c8a44b155a41edb57b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "167a132b5f7046e1a68a7b1b0b74655e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77a29d7b7d9b407fbdea433b332ef40f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9dad6a89914d482d8dec50dde17693fa",
      "value": "â€‡323M/323Mâ€‡[00:09&lt;00:00,â€‡72.8MB/s]"
     }
    },
    "17fad981500343a3af299350110835bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "192b60aa46c84d18be1e64595b0d72ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1995152b18554285b0bea612c9d9a7d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1a6a83cdb28b4d12a8e73bee2e289bc7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a2ad2000fb864b0198eb51d6fcdfd67f",
       "IPY_MODEL_819ce1954fd2480fbd3c7f8f6271c640",
       "IPY_MODEL_1086c939e66a40bba65b5a9bdf6040ac"
      ],
      "layout": "IPY_MODEL_d351dbc58226406e9b9a7c0d52525c62"
     }
    },
    "1af677e28d064f76b76afae01ca22f36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_795afc4d1542495785d43eea36953a10",
      "max": 1192135096,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8979ccdb08df4ec49972b1f1c21c00c9",
      "value": 1192135096
     }
    },
    "1f3ac5a063ba406daa9317b1b7fce19f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b8121af4c23448f7bd3c2c639ce96286",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_491b66bd00284502ac7e8997014a7c59",
      "value": "â€‡1.21k/?â€‡[00:00&lt;00:00,â€‡141kB/s]"
     }
    },
    "1f5cb315b409433898d87dcf2b1b59ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "23183630ed4c490989d10dab3ad8516b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "249110bb758a48bab27e5c11cd42d532": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25d1803806d943e6af8c4420a697be00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26317768f08a4da19ff638dbb6fd6f8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2aba542f61914328a820a75323825822": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bf0edb9ccd654fc4ad2e87913d6ae4ca",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f9fe5f5747f24f3d9c603ce962d34bf8",
      "value": "vocab.json:â€‡"
     }
    },
    "2d914086caaf499fbb58b49adefbdd88": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30409042e6f94ca7bc6c2b0828e4c4d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "337b9d46148e4aff807a65ed9711d974": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fcd4b5304334748b43f00f25c49b9cd",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3ae7999286c9426aa6fdcd9f4fe1513b",
      "value": 1
     }
    },
    "38ef6225668e4941b8f9c52b44fb7921": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_757a9ad63df242ab9a82f887bc40e339",
       "IPY_MODEL_1af677e28d064f76b76afae01ca22f36",
       "IPY_MODEL_394feb3014f0404aacc8336c91092a0e"
      ],
      "layout": "IPY_MODEL_109f2f4986df40238a6624039d90c4e1"
     }
    },
    "394feb3014f0404aacc8336c91092a0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f6ec84ce5664145920628847640af01",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ddf68d642b6748059696d911a52bd3d6",
      "value": "â€‡1.19G/1.19Gâ€‡[00:03&lt;00:00,â€‡712MB/s]"
     }
    },
    "3ae7999286c9426aa6fdcd9f4fe1513b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3e9384fad734470f9e082a8608b94af9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "402663dd3356403998d8027a6148cebb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bb40354af17a49efb38d2147fe229536",
       "IPY_MODEL_e1620b46125b4a79bab053d5ba6a460b",
       "IPY_MODEL_c0d05dca5fde4ff49b4ca72b390457ec"
      ],
      "layout": "IPY_MODEL_ed7bfea8f8874240b402e80351b909ae"
     }
    },
    "47d9d78b00664c50a0eca40b5900e19c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "491b66bd00284502ac7e8997014a7c59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4bb030af9dc6475a9a8818a5884a611a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd9d55a0d1084f66b286ebaccd3db133",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a0f525c972114b13b7d2a5138c6cf0f8",
      "value": "â€‡138/138â€‡[00:00&lt;00:00,â€‡18.1kB/s]"
     }
    },
    "4edf7a2bd1ab48f5824bd5dac3ae2277": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d914086caaf499fbb58b49adefbdd88",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_dc1dec119a0c49afa8328d7b4263599e",
      "value": "â€‡1.67M/?â€‡[00:00&lt;00:00,â€‡273kB/s]"
     }
    },
    "4f6ec84ce5664145920628847640af01": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5653a6a266a345f6aa7afa7753755e29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5be6cabc0cc44aad8a0c2ccc6c449648": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "61105698b7e24b27a540f90ec9e6b8eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b0273a02d0944bd9a734ecab978cab2b",
       "IPY_MODEL_995cf8bb48744f53b5ab8b2ac2022a28",
       "IPY_MODEL_4edf7a2bd1ab48f5824bd5dac3ae2277"
      ],
      "layout": "IPY_MODEL_f6d9f4f53dc64863bbd65ff5a90eeedc"
     }
    },
    "627cd391e0cf4789b8dd4a8619ef1880": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6676099344d4451eb0945fef5330dbeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c060a6831e4490287d91fa8339b46ed",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e1adc63ae3c84fab9bee6676f52fee2e",
      "value": "â€‡727/727â€‡[00:00&lt;00:00,â€‡95.6kB/s]"
     }
    },
    "695356b4c3e747f3a6e5ce0a0f2a8a1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1656cc6a6ad449c8a44b155a41edb57b",
      "max": 727,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_627cd391e0cf4789b8dd4a8619ef1880",
      "value": 727
     }
    },
    "6b26b036b06d47a99b26b4fa45afd847": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ba49fa24fb94a9dbce271fb6646705b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_856c2859229c4805b1f0b5fdc5d52c82",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_30409042e6f94ca7bc6c2b0828e4c4d4",
      "value": "adapter_config.json:â€‡"
     }
    },
    "6f6952f396ed412c80182c7a8a20bb7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f611bcf34c5a422bb05d3eed88cbad83",
       "IPY_MODEL_8f877aace13341e194243a8905e392dd",
       "IPY_MODEL_4bb030af9dc6475a9a8818a5884a611a"
      ],
      "layout": "IPY_MODEL_6b26b036b06d47a99b26b4fa45afd847"
     }
    },
    "7345e26939664417a06c6f2b0cfd752c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74af14ac05fa4d568fbebf8ce34fecda": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "757a9ad63df242ab9a82f887bc40e339": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b5fe1aae33684b05a476f83361ac2836",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5653a6a266a345f6aa7afa7753755e29",
      "value": "model.safetensors:â€‡100%"
     }
    },
    "77a29d7b7d9b407fbdea433b332ef40f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "795afc4d1542495785d43eea36953a10": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "819ce1954fd2480fbd3c7f8f6271c640": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0ae5d477d944de0b4be6e54db6d28f6",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_07ed789a716146a19d18646dc4c4f831",
      "value": 1
     }
    },
    "828c8ddffef84a8999b6a54a940dff87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "851ae2f59e9e48dc908841a4a86a3162": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "856c2859229c4805b1f0b5fdc5d52c82": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8979ccdb08df4ec49972b1f1c21c00c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8a29849920c64228ac8c3d56ae646671": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b01f05fe53e49ada6b7a063d014bf5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8bfdae4bb8dd415ab930bdd9bb0ec979": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8c060a6831e4490287d91fa8339b46ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d46816088454f2a85d7ccc923112331": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f877aace13341e194243a8905e392dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_851ae2f59e9e48dc908841a4a86a3162",
      "max": 138,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1410e2b327af49c79409448ea407c33b",
      "value": 138
     }
    },
    "995cf8bb48744f53b5ab8b2ac2022a28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_47d9d78b00664c50a0eca40b5900e19c",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f1324e82f67f4804a4aa7334db511da1",
      "value": 1
     }
    },
    "9c64e9dc4a734355be2a2785b731ad31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e62ec967afb54167ab9246d99879ace9",
       "IPY_MODEL_c73ee4e19fd4408996bdef9cd48de335",
       "IPY_MODEL_167a132b5f7046e1a68a7b1b0b74655e"
      ],
      "layout": "IPY_MODEL_ea6395a0075d40bea56f9687776251ce"
     }
    },
    "9dad6a89914d482d8dec50dde17693fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a01cbebb72a24440ab6a54eb9d7ad904": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a049c1ec4a884d0cbae526ac89b7f014": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3cdc83526d744e491f2acfbaa7551c5",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d46ba69fe1774ccf85887f98b1660c01",
      "value": 1
     }
    },
    "a0f525c972114b13b7d2a5138c6cf0f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a2ad2000fb864b0198eb51d6fcdfd67f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_249110bb758a48bab27e5c11cd42d532",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1995152b18554285b0bea612c9d9a7d8",
      "value": "tokenizer.json:â€‡"
     }
    },
    "a9c078359c5049969603475a7c41aa6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aa6e955056ea4734a9818ee6ee664e84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6ba49fa24fb94a9dbce271fb6646705b",
       "IPY_MODEL_337b9d46148e4aff807a65ed9711d974",
       "IPY_MODEL_1f3ac5a063ba406daa9317b1b7fce19f"
      ],
      "layout": "IPY_MODEL_25d1803806d943e6af8c4420a697be00"
     }
    },
    "aaa9e8f1633b4e079f0488e21fbff8fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2aba542f61914328a820a75323825822",
       "IPY_MODEL_a049c1ec4a884d0cbae526ac89b7f014",
       "IPY_MODEL_0719a1cfede94b55a1ba28985200016f"
      ],
      "layout": "IPY_MODEL_7345e26939664417a06c6f2b0cfd752c"
     }
    },
    "b0273a02d0944bd9a734ecab978cab2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_74af14ac05fa4d568fbebf8ce34fecda",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c20b7e1ee8e74617866873dbbe5520f8",
      "value": "merges.txt:â€‡"
     }
    },
    "b5fe1aae33684b05a476f83361ac2836": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b8121af4c23448f7bd3c2c639ce96286": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "baa530a832b34832a96d5c26ce130b15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb40354af17a49efb38d2147fe229536": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d46816088454f2a85d7ccc923112331",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8a29849920c64228ac8c3d56ae646671",
      "value": "tokenizer_config.json:â€‡"
     }
    },
    "bd9d55a0d1084f66b286ebaccd3db133": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf0edb9ccd654fc4ad2e87913d6ae4ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0d05dca5fde4ff49b4ca72b390457ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a01cbebb72a24440ab6a54eb9d7ad904",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8bfdae4bb8dd415ab930bdd9bb0ec979",
      "value": "â€‡9.68k/?â€‡[00:00&lt;00:00,â€‡1.21MB/s]"
     }
    },
    "c20b7e1ee8e74617866873dbbe5520f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c73ee4e19fd4408996bdef9cd48de335": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_828c8ddffef84a8999b6a54a940dff87",
      "max": 323014520,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_26317768f08a4da19ff638dbb6fd6f8a",
      "value": 323014520
     }
    },
    "cf5e94222def4bb29d74948fa23928e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d351dbc58226406e9b9a7c0d52525c62": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d46ba69fe1774ccf85887f98b1660c01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dc1dec119a0c49afa8328d7b4263599e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dcff1d7da31f47cba4e68e30dace7d03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ddf68d642b6748059696d911a52bd3d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e0ae5d477d944de0b4be6e54db6d28f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "e1620b46125b4a79bab053d5ba6a460b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f5cb315b409433898d87dcf2b1b59ba",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_17fad981500343a3af299350110835bf",
      "value": 1
     }
    },
    "e1adc63ae3c84fab9bee6676f52fee2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e3cdc83526d744e491f2acfbaa7551c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "e6210bb59e7f4191a5347284a9379d1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e62ec967afb54167ab9246d99879ace9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6210bb59e7f4191a5347284a9379d1b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_dcff1d7da31f47cba4e68e30dace7d03",
      "value": "adapter_model.safetensors:â€‡100%"
     }
    },
    "ea6395a0075d40bea56f9687776251ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed7bfea8f8874240b402e80351b909ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1324e82f67f4804a4aa7334db511da1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f611bcf34c5a422bb05d3eed88cbad83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23183630ed4c490989d10dab3ad8516b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_cf5e94222def4bb29d74948fa23928e8",
      "value": "generation_config.json:â€‡100%"
     }
    },
    "f6d9f4f53dc64863bbd65ff5a90eeedc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f858797894bd4a0cac2326d9da9b588c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0ff5e66c67da4db5aa4ba1317fa6b738",
       "IPY_MODEL_695356b4c3e747f3a6e5ce0a0f2a8a1e",
       "IPY_MODEL_6676099344d4451eb0945fef5330dbeb"
      ],
      "layout": "IPY_MODEL_3e9384fad734470f9e082a8608b94af9"
     }
    },
    "f9542acd04754b359d305cdee4da97e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9fe5f5747f24f3d9c603ce962d34bf8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
